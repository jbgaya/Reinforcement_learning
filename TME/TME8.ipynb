{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import gym\n",
    "import multiagent\n",
    "import multiagent.scenarios\n",
    "import multiagent.scenarios.simple_tag as simple_tag\n",
    "import multiagent.scenarios.simple_tag as simple_spread\n",
    "import multiagent.scenarios.simple_tag as simple_adversary\n",
    "from multiagent.environment import MultiAgentEnv\n",
    "import multiagent.scenarios as scenarios\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "import copy\n",
    "from multiagent.environment import MultiAgentEnv\n",
    "import multiagent.scenarios as scenarios\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import copy\n",
    "\n",
    "from random import sample\n",
    "from collections import deque\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(obs):\n",
    "    return torch.Tensor(obs).to(device,torch.double)\n",
    "\n",
    "def UONoise():\n",
    "    theta = 0.15\n",
    "    sigma = 0.3\n",
    "    state = 0\n",
    "    while True:\n",
    "        yield state\n",
    "        state += -theta*state+sigma*np.random.randn()\n",
    "\n",
    "class NN_Q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_Q, self).__init__()\n",
    "        self.f1 = nn.Linear(14,100)\n",
    "        self.f2 = nn.Linear(106,100)\n",
    "        self.f3 = nn.Linear(100,1)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(14)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        \n",
    "    def forward(self, x , actions ):\n",
    "        x = self.bn0(x)\n",
    "        x = torch.relu(self.f1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.cat((x,actions),dim=1)\n",
    "        x = torch.relu(self.f2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.f3(x)\n",
    "        return x\n",
    "\n",
    "class NN_mu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_mu, self).__init__()\n",
    "        self.f1 = nn.Linear(14,100)\n",
    "        self.f2 = nn.Linear(100,2)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(14)        \n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        \n",
    "    def forward(self, x ):\n",
    "        x = self.bn0(x)\n",
    "        x = torch.relu(self.f1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.tanh(self.f2(x))\n",
    "        return x\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, N=1000000):\n",
    "        self.data = deque(maxlen=N)\n",
    "        \n",
    "    def sample(self,n):\n",
    "        samples = sample(self.data,n)\n",
    "        lastobs = torch.stack([item[0] for item in samples],dim=1)\n",
    "        action = torch.stack([item[1] for item in samples],dim=0).to(device,torch.double)\n",
    "        r = torch.stack([item[2] for item in samples],dim=0)\n",
    "        obs = torch.stack([item[3] for item in samples],dim=1)\n",
    "        done = torch.Tensor([item[4] for item in samples]).unsqueeze(1).to(device)\n",
    "        \n",
    "        return lastobs,action,r,obs,done\n",
    "            \n",
    "    \n",
    "    def store(self,last_obs,a,r,obs,done):\n",
    "        self.data.append([last_obs,a,r,obs,done])\n",
    "\n",
    "class DDPG_agent():\n",
    "    def __init__(self):\n",
    "        \n",
    "        #Creating Q functions\n",
    "        self.Q = NN_Q().to(device,torch.double)\n",
    "        self.Q_target = NN_Q().to(device,torch.double)\n",
    "        self.Q_loss = nn.MSELoss()\n",
    "        self.Q_target.load_state_dict(self.Q.state_dict())\n",
    "        self.opt_Q = torch.optim.Adam(self.Q.parameters(),lr=0.01)\n",
    "        \n",
    "        #Creating mu functions\n",
    "        self.mu = NN_mu().to(device,torch.double)\n",
    "        self.mu_target = NN_mu().to(device,torch.double)\n",
    "        self.mu_target.load_state_dict(self.mu.state_dict())\n",
    "        self.opt_mu = torch.optim.Adam(self.mu.parameters(),lr=0.01)\n",
    "        self.mu.eval()\n",
    "        \n",
    "        #Noise handling\n",
    "        self.explo_1 = UONoise()\n",
    "        self.explo_2 = UONoise()\n",
    "        \n",
    "    \n",
    "    def act(self,obs,explo):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #action = self.mu.forward(obs.unsqueeze(0)).to(torch.device(\"cpu\")) + torch.Tensor(np.random.normal(size=2))*explo\n",
    "            action = self.mu.forward(obs.unsqueeze(0)).to(torch.device(\"cpu\")) +\\\n",
    "            torch.Tensor([next(self.explo_1),next(self.explo_2)])*explo\n",
    "            \n",
    "        return np.array(action).reshape(-1)\n",
    "    \n",
    "class Multi_agent():\n",
    "    def __init__(self,num_agents=3,tau=0.1,gamma=0.95,batch=1024,update_freq=1,max_explo=250,epochs=1,start_train=150):\n",
    "    \n",
    "        self.agents = [DDPG_agent() for _ in range(3)]\n",
    "        \n",
    "        self.episodes = 0\n",
    "        self.update_freq = update_freq\n",
    "        self.max_explo = max_explo\n",
    "        self.start_train = start_train\n",
    "        self.i = 0\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.memory = Memory()\n",
    "        self.batch = batch\n",
    "        \n",
    "    def update(self):\n",
    "        for agent in self.agents:\n",
    "            agent.mu.train()\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            self.i +=1\n",
    "            lastobs,actions,r,obs,done = self.memory.sample(self.batch)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mu = torch.cat([agent.mu_target.forward(obs[i]) for i,agent in enumerate(self.agents)],dim=1)\n",
    "\n",
    "            mu_last = [torch.cat([actions[:,:i*2],agent.mu.forward(lastobs[i]),actions[:,(i+1)*2:]],dim=1) for i,agent in enumerate(self.agents)]\n",
    "            for i,agent in enumerate(self.agents):\n",
    "                \n",
    "                #1- Critic update (Q)\n",
    "                agent.opt_Q.zero_grad()\n",
    "                agent.opt_mu.zero_grad()\n",
    "                with torch.no_grad():\n",
    "                    y = r + self.gamma * agent.Q_target.forward(obs[i],mu)\n",
    "                Qloss = agent.Q_loss( y , agent.Q.forward(lastobs[i],actions) )\n",
    "                writer.add_scalar('QLoss_'+str(i),Qloss.item(),self.i)\n",
    "                Qloss.backward()\n",
    "                agent.opt_Q.step()\n",
    "\n",
    "\n",
    "                #2- Actor update (mu)\n",
    "                agent.opt_Q.zero_grad()\n",
    "                agent.opt_mu.zero_grad()\n",
    "                mu_loss = -agent.Q.forward( lastobs[i] , mu_last[i] ).mean()\n",
    "                writer.add_scalar('mu_loss_'+str(i),-mu_loss.item(),self.i)\n",
    "                mu_loss.backward()\n",
    "                agent.opt_mu.step()\n",
    "\n",
    "        #3- smooth update of Q and mu\n",
    "        for agent in self.agents:\n",
    "            for p_target,p in zip(agent.Q_target.parameters(),agent.Q.parameters()):\n",
    "                p_target.data.copy_( self.tau * p.data + (1-self.tau) * p_target.data )\n",
    "            for p_target,p in zip(agent.mu_target.parameters(),agent.mu.parameters()):\n",
    "                p_target.data.copy_( self.tau * p.data + (1-self.tau) * p_target.data )\n",
    "            \n",
    "        for agent in self.agents:\n",
    "            agent.mu.eval()   \n",
    "        \n",
    "        \n",
    "    def act(self,obs,done):\n",
    "        \n",
    "        explo = 1 - min(self.episodes,self.max_explo) / self.max_explo\n",
    "        actions = [agent.act(o,explo) for agent,o in zip(self.agents,obs)]\n",
    "        \n",
    "        if self.episodes%self.update_freq==0 and self.episodes>self.start_train:\n",
    "            self.update()     \n",
    "        \n",
    "        return actions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(scenario_name, benchmark=False):\n",
    "    '''\n",
    "    Creates a MultiAgentEnv object as env. This can be used similar to a gym\n",
    "    environment by calling env.reset() and env.step().\n",
    "    Use env.render() to view the environment on the screen.\n",
    "\n",
    "    Input:\n",
    "        scenario_name   :   name of the scenario from ./scenarios/ to be Returns\n",
    "                            (without the .py extension)\n",
    "        benchmark       :   whether you want to produce benchmarking data\n",
    "                            (usually only done during evaluation)\n",
    "\n",
    "    Some useful env properties (see environment.py):\n",
    "        .observation_space  :   Returns the observation space for each agent\n",
    "        .action_space       :   Returns the action space for each agent\n",
    "        .n                  :   Returns the number of Agents\n",
    "    '''\n",
    "    # load scenario from script\n",
    "    scenario = scenarios.load(scenario_name + \".py\").Scenario()\n",
    "    # create world\n",
    "    world = scenario.make_world()\n",
    "    # create multiagent environment\n",
    "    world.dim_c = 0\n",
    "    if benchmark:\n",
    "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation, scenario.benchmark_data)\n",
    "    else:\n",
    "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)\n",
    "    env.discrete_action_space = False\n",
    "    env.discrete_action_input = False\n",
    "    scenario.reset_world(world)\n",
    "    return env,scenario,world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  50 :  r = -5.5\n",
      "Episode  100 :  r = -3.7\n",
      "Episode  150 :  r = -6.1\n",
      "Episode  200 :  r = -4.7\n",
      "Episode  250 :  r = -2.5\n",
      "Episode  300 :  r = -3.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-222c46859cc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlastobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5914f5511844>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, done)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mexplo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_explo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_explo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexplo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_freq\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5914f5511844>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mexplo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_explo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_explo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexplo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_freq\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5914f5511844>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, explo)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m#action = self.mu.forward(obs.unsqueeze(0)).to(torch.device(\"cpu\")) + torch.Tensor(np.random.normal(size=2))*explo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplo_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplo_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexplo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5914f5511844>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env,scenario,world = make_env('simple_spread')\n",
    "writer = SummaryWriter(\"runs/multiagent/simple_spread\")\n",
    "nb_agents = 3\n",
    "agent = Multi_agent(num_agents=nb_agents)\n",
    "nb_episodes = 1000\n",
    "\n",
    "\n",
    "\n",
    "for i in range(nb_episodes):\n",
    "    obs = env.reset()\n",
    "    lastobs = None\n",
    "    reward = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        a = agent.act(phi(obs),False)\n",
    "        obs, r, _, _ = env.step(a)\n",
    "        if lastobs:\n",
    "            agent.memory.store(phi(lastobs),\n",
    "                               torch.Tensor(a).to(device).reshape(-1),\n",
    "                               torch.Tensor([r[0]]).to(device,torch.double),\n",
    "                               phi(obs),False)\n",
    "\n",
    "        lastobs = obs\n",
    "        reward.append(r)\n",
    "        #if (i+1)%101==0:\n",
    "            #env.render(mode=\"none\")\n",
    "    writer.add_scalar('rewards',round(sum([sum(R) for R in reward])/1000,1),i)\n",
    "    if (i+1)%50==0:\n",
    "        print(\"Episode \",i+1,\": \",\"r =\",round(sum([sum(R) for R in reward])/1000,1))\n",
    "    agent.episodes += 1\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
