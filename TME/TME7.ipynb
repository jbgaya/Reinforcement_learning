{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import math\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import gym\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "    \n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import copy\n",
    "\n",
    "from random import sample\n",
    "from collections import deque\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device : \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UONoise():\n",
    "    theta = 0.15\n",
    "    sigma = 0.8\n",
    "    state = 0\n",
    "    while True:\n",
    "        yield state\n",
    "        state += -theta*state+sigma*np.random.randn()\n",
    "\n",
    "class NN_Q(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_Q, self).__init__()\n",
    "        self.f1 = nn.Linear(2,100)\n",
    "        self.f2 = nn.Linear(101,150)\n",
    "        self.f3 = nn.Linear(150,1)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(2)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.bn2 = nn.BatchNorm1d(150)\n",
    "        \n",
    "    def forward(self, x , action ):\n",
    "        x = self.bn0(x)\n",
    "        x = torch.relu(self.f1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.cat((x,action),dim=1)\n",
    "        x = torch.relu(self.f2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.f3(x)\n",
    "        return x\n",
    "\n",
    "class NN_mu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN_mu, self).__init__()\n",
    "        self.f1 = nn.Linear(2,50)\n",
    "        self.f2 = nn.Linear(50,25)\n",
    "        self.f3 = nn.Linear(25,1)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(2)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.bn2 = nn.BatchNorm1d(25)\n",
    "        \n",
    "    def forward(self, x ):\n",
    "        x = self.bn0(x)\n",
    "        x = torch.relu(self.f1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(self.f2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.tanh(self.f3(x))\n",
    "        return x\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, N=500000):\n",
    "        self.data = deque(maxlen=N)\n",
    "        \n",
    "    def sample(self,n):\n",
    "        samples = sample(self.data,n)\n",
    "        lastobs = torch.stack([item[0] for item in samples],dim=0)\n",
    "        action = torch.cat([item[1] for item in samples],dim=0)\n",
    "        r = torch.stack([item[2] for item in samples],dim=0)\n",
    "        obs = torch.stack([item[3] for item in samples],dim=0)\n",
    "        done = torch.Tensor([item[4] for item in samples]).unsqueeze(1).to(device)\n",
    "        \n",
    "        return lastobs,action,r,obs,done\n",
    "            \n",
    "    \n",
    "    def store(self,last_obs,a,r,obs,done):\n",
    "        self.data.append([last_obs,a,r,obs,done])\n",
    "\n",
    "class DDPG_agent():\n",
    "    def __init__(self,tau=0.001,gamma=0.99,batch=64,update_freq=1,max_explo=200,epochs=1,start_train=1):\n",
    "        \n",
    "        #Creating Q functions\n",
    "        self.Q = NN_Q().to(device,torch.double)\n",
    "        self.Q_target = NN_Q().to(device,torch.double)\n",
    "        self.Q_loss = nn.MSELoss()\n",
    "        self.Q_target.load_state_dict(self.Q.state_dict())\n",
    "        self.opt_Q = torch.optim.Adam(self.Q.parameters(),lr=0.001)\n",
    "        \n",
    "        #Creating mu functions\n",
    "        self.mu = NN_mu().to(device,torch.double)\n",
    "        self.mu_target = NN_mu().to(device,torch.double)\n",
    "        self.mu_target.load_state_dict(self.mu.state_dict())\n",
    "        self.opt_mu = torch.optim.Adam(self.mu.parameters(),lr=0.0001)\n",
    "        self.mu.eval()\n",
    "        \n",
    "        #Memory storage\n",
    "        self.memory = Memory()\n",
    "        self.last_obs = torch.zeros(0)\n",
    "        self.last_a = None\n",
    "        \n",
    "        #Hyperparameters\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.batch = batch\n",
    "        self.update_freq = update_freq\n",
    "        self.epochs = epochs\n",
    "        self.start_train = start_train\n",
    "        \n",
    "        #Noise handling and indexing\n",
    "        self.max_explo = max_explo\n",
    "        self.episodes = 0\n",
    "        self.exploration = UONoise() #variance of normal distribution used for exploration\n",
    "        self.i = 0                   #nb of updates\n",
    "        \n",
    "    def phi(self,obs):\n",
    "        return torch.Tensor(obs).to(device,torch.double)\n",
    "        \n",
    "    def update(self):\n",
    "        self.mu.train()\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            self.i +=1\n",
    "            lastobs,action,r,obs,done = self.memory.sample(self.batch)\n",
    "               \n",
    "            #1- Critic update (Q)\n",
    "            self.opt_Q.zero_grad()\n",
    "            self.opt_mu.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                y = r + self.gamma * (1-done) * self.Q_target.forward(obs,self.mu_target.forward(obs))\n",
    "            Qloss = self.Q_loss( y , self.Q.forward(lastobs,action) )\n",
    "            writer.add_scalar('QLoss',Qloss.item(),self.i)\n",
    "            Qloss.backward()\n",
    "            self.opt_Q.step()\n",
    "\n",
    "\n",
    "            #2- Actor update (mu)\n",
    "            self.opt_Q.zero_grad()\n",
    "            self.opt_mu.zero_grad()\n",
    "            mu_loss = -self.Q.forward( lastobs , self.mu.forward(lastobs) ).mean()\n",
    "            writer.add_scalar('mu_loss',-mu_loss.item(),self.i)\n",
    "            mu_loss.backward()\n",
    "            self.opt_mu.step()\n",
    "\n",
    "        #3- smooth update of Q and mu\n",
    "        for p_target,p in zip(self.Q_target.parameters(),self.Q.parameters()):\n",
    "            p_target.data.copy_( self.tau * p.data + (1-self.tau) * p_target.data )\n",
    "        for p_target,p in zip(self.mu_target.parameters(),self.mu.parameters()):\n",
    "            p_target.data.copy_( self.tau * p.data + (1-self.tau) * p_target.data )\n",
    "            \n",
    "        self.mu.eval()\n",
    "\n",
    "        \n",
    "    \n",
    "    def act(self,obs,r,done):\n",
    "        obs = self.phi(obs)\n",
    "        r = torch.Tensor([r]).to(device,torch.double)\n",
    "        exploration = 1 - min(self.episodes,self.max_explo) / self.max_explo\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = torch.clamp(self.mu.forward(obs.unsqueeze(0)) + next(self.exploration)*exploration ,-1 ,1 )\n",
    "                 \n",
    "        \n",
    "        if self.last_obs.shape[0] != 0:\n",
    "            self.memory.store(self.last_obs,self.last_a,r,obs,done)\n",
    "            \n",
    "        if self.episodes%self.update_freq==0 and self.episodes>self.start_train:\n",
    "            self.update()\n",
    "        \n",
    "        if done:\n",
    "            self.episodes +=1\n",
    "        \n",
    "        self.last_obs = obs\n",
    "        self.last_a = action\n",
    "        return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training phase on  5000  episodes :\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 8 elements not 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5969725188cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrsum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-187c996c4df9>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, obs, r, done)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexploration\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-187c996c4df9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 8 elements not 2"
     ]
    }
   ],
   "source": [
    "# Execution avec un Agent Q-learning\n",
    "writer = SummaryWriter(\"runs/LunarLanderContinuous-v2/DDPG\")\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "agent = DDPG_agent()\n",
    "env.seed(0)\n",
    "reward = 0\n",
    "done = False\n",
    "rsum = 0\n",
    "episode_count = 5000\n",
    "\n",
    "#Training phase\n",
    "print(\"Starting training phase on \",episode_count,\" episodes :\")\n",
    "for i in range(1,episode_count+1):\n",
    "    obs = env.reset()\n",
    "    j = 0\n",
    "    rsum = 0\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(obs,reward,done)\n",
    "        obs, reward, done, _ = env.step([action])\n",
    "        rsum += reward\n",
    "        j += 1\n",
    "        if (i%50==0):\n",
    "            env.render()\n",
    "        if done:\n",
    "            writer.add_scalar('train rewards',rsum,i)\n",
    "            print(\"Episode : \" + str(i) + \" rsum=\" + str(round(rsum,2)) + \", \" + str(j) + \" actions\")\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
